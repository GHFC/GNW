# -*- coding: utf-8 -*-
"""Local_server.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1klD4KSCMlSfBEc8_1TKjU-DXpV3steeJ
"""

from skimage.util import img_as_float
from skimage.io import imread
import numpy as np
from skimage.transform import resize
from skimage.transform import rescale
from copy import copy
import pickle
import matplotlib
import os
from skimage.color import rgb2gray
from skimage.color import hsv2rgb
import random
import matplotlib.pyplot as plt
from brian2 import *
from IPython.display import clear_output
import progressbar
from scipy import ndimage as nd
from random import shuffle
from skimage.filters import gabor_kernel

imgDef = 128
receptiveFieldSize=int(imgDef/18) #initial 16
C1num = len(range(0,imgDef-receptiveFieldSize,receptiveFieldSize-2))

# Local inhibition

def power_conv(image, kernel):
    # Normalize images for better comparison.
    image = (image - image.mean()) / image.std()
    return np.sqrt(nd.convolve(image, np.real(kernel), mode='nearest')**2 +
                   nd.convolve(image, np.imag(kernel), mode='nearest')**2)

def makeGaussian(size, fwhm = 3, center=None):
    """ Make a square gaussian kernel.

    size is the length of a side of the square
    fwhm is full-width-half-maximum, which
    can be thought of as an effective radius.
    """

    x = np.arange(0, size, 1, dtype=float)
    y = x[:,np.newaxis]

    if center is None:
        x0 = y0 = size // 2
    else:
        x0 = center[0]
        y0 = center[1]

    return np.exp(-4*np.log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)

kernelInhibition=makeGaussian(11, fwhm = 5)*1.15
kernelInhibition[5,5]=0

sigma_values = [1]
frequency_values = [0.1]

kernels = []
for theta in range(6):
    theta = (theta / 6. + 1/8) * np.pi
    for sigma in sigma_values:
        for frequency in frequency_values:
            kernel = gabor_kernel(frequency, theta=theta,
                                          sigma_x=sigma, sigma_y=sigma)
            #print(kernel.shape)
            kernels.append(kernel)

## Dataset Upload ##
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

specific_nums = []
for num in range(10):
    specific_nums.append(x_train[y_train == num])
objects = []

digits = 2
num_nums_specific = int((2000 + digits - 1) / digits)
num_nums_specific_valid = int(100 / digits)
for num in range(digits): #num of objects used in the input
    for numnum in range(num_nums_specific + num_nums_specific_valid):
        objects.append(rescale(specific_nums[num][numnum], 128/28, multichannel=False))

## S1 and C1 Layers ##
num_obj = 3501

single = False
pickles = []

#for cnt,img in enumerate([objects[num_obj]]):
for cnt in progressbar.progressbar(range(len(objects))):
    #print('Current file processing: ', cnt)
    if single:
        cnt = num_obj
    img = objects[cnt]
    if single:
        figG, axesG = plt.subplots(nrows=1, ncols=6, figsize=(24, 4))
        figG.suptitle('Gabor convolutions at different angles (S1 layer)', fontsize=24)
    spikes=np.zeros((imgDef,imgDef,6,len(sigma_values),len(frequency_values)))
    n=0
    for theta in range(6):
        for s,sigma in enumerate(sigma_values):
            for f,frequency in enumerate(frequency_values):
                spikes[:,:,theta,s,f]=resize(power_conv(img,kernels[n]), (imgDef, imgDef)) #convolutions for all thetas, sigmas, f
                if single:
                    ax = axesG[n]
                    ax.imshow(spikes[:,:,theta,s,f], interpolation='none')
                    ax.axis('off')
                n+=1

    S1ori=np.argmax(spikes, axis=2) #number of orientation propagated (C1xC1xsxf)
    S1val=np.max(spikes, axis=2) #value at this orientation
    S1ori[S1val < 0.5 * np.max(S1val)] = -1 #EDIT to cancel out cells where actually nothing but they still got some orientation (3e-1)
    #print(S1val.shape)
    C1ori=np.zeros([C1num,C1num,len(sigma_values),len(frequency_values)])
    C1lat=np.zeros([C1num,C1num,len(sigma_values),len(frequency_values)])
    if single:
        fig1, axes1 = plt.subplots(nrows=len(frequency_values), ncols=len(sigma_values), figsize=(5, 3))
        fig1.suptitle('C1 Layer - Orientation #'+str(cnt), fontsize=12);n=0
        fig2, axes2 = plt.subplots(nrows=len(frequency_values), ncols=len(sigma_values), figsize=(5, 3))
        fig2.suptitle('C1 Layer - Latency before lateral inhibition#'+str(cnt), fontsize=12);n=0
        fig3, axes3 = plt.subplots(nrows=len(frequency_values), ncols=len(sigma_values), figsize=(5, 3))
        fig3.suptitle('C1 Layer - Latency after lateral inhibition#'+str(cnt), fontsize=12);n=0
    for s,sigma in enumerate(sigma_values):
        for f,frequency in enumerate(frequency_values):
            for x,px in enumerate(range(0,imgDef-receptiveFieldSize,receptiveFieldSize-2)):
                for y,py in enumerate(range(0,imgDef-receptiveFieldSize,receptiveFieldSize-2)):
                    fstSpX,fstSpY=np.unravel_index(np.argmax(S1val[px:(px+receptiveFieldSize),py:(py+receptiveFieldSize),s,f]),(receptiveFieldSize,receptiveFieldSize))
                    C1ori[x,y,s,f]=S1ori[px+fstSpX,py+fstSpY,s,f]
                    v=S1val[px+fstSpX,py+fstSpY,s,f]
                    #print(1/v)
                    if v==0:
                        C1lat[x,y,s,f]=np.nan
                        C1ori[x,y,s,f]=np.nan
                    else:
                        C1lat[x,y,s,f]=1/v
            if single:
                ax1 = axes1#[f][s]
                im1=ax1.imshow(C1ori[:,:,s,f])#, vmin=0, vmax=6,interpolation='none')
                ax1.axis('off');
                ax2 = axes2#[f][s]
                im2=ax2.imshow(C1lat[:,:,s,f], vmin=0,interpolation='none')
                ax2.axis('off');n+=1
    if single:
        fig1.subplots_adjust(right=0.8)
        cbar_ax = fig1.add_axes([0.85, 0.15, 0.05, 0.7])
        fig1.colorbar(im1, cax=cbar_ax)
        fig2.subplots_adjust(right=0.8)
        cbar_ax = fig2.add_axes([0.85, 0.15, 0.05, 0.7])
        fig2.colorbar(im2, cax=cbar_ax)
    
    flag_inhib = False
    
    for s,sigma in enumerate(sigma_values):
        for f,frequency in enumerate(frequency_values):
            tmp2=copy(C1lat[:,:,s,f])
            #print(np.amax(C1lat[:,:,s,f]))
            for ori in range(6):
                tmp=np.zeros((C1num,C1num))
                ori_indices = (C1ori[:,:,s,f]==ori)
                tmp[ori_indices] = 1 # binary array: whether it is current orientation or not
                tmp=nd.convolve(tmp, kernelInhibition, mode='wrap') #, mode='constant', cval=0.0)
                tmp2[ori_indices] = clip(tmp2[ori_indices] * tmp[ori_indices], tmp2[ori_indices], None)
            if not flag_inhib:
                C1lat[:,:,s,f]=copy(tmp2)
                flag_inhib = True
                #print(np.amax(C1lat[:,:,s,f]), np.amax(tmp2))
            if single:
                ax3 = axes3#[f][s]
                im3=ax3.imshow(C1lat[:,:,s,f], vmin=0,interpolation='none')
                ax3.axis('off');
                fig3.subplots_adjust(right=0.8)
                cbar_ax = fig3.add_axes([0.85, 0.15, 0.05, 0.7])
                fig3.colorbar(im3, cax=cbar_ax)

    
    ## Saving the C1 layer for the first Gabor filter ##
    
    C1 = np.zeros([C1num, C1num, 6])
    for ori in range(6):
        tmp=np.zeros((C1num,C1num))
        tmp2=copy(C1lat[:,:,0,0])
        tmp[np.nonzero(C1ori[:,:,0,0]==ori)] = tmp2[np.nonzero(C1ori[:,:,0,0]==ori)]
        C1[:, :, ori] = copy(tmp)
    
    if single:    
        cols = 6
        n = 0
        fig4, axes4 = plt.subplots(nrows=1, ncols=cols, figsize=(24, 4))
        fig4.suptitle('C1 Layer sparse, orientations - Latency after lateral inhibition#'+str(cnt), fontsize=24)
        for y in range(cols):
            ax4 = axes4[y]
            ax4.imshow(C1[:, :, n], vmin=0, interpolation='none')
            ax4.axis('off');
            n+=1

    ## Saving the objects: ##
    
    if not single:
          pickles.append([C1, C1num])
    
    if single:
        break
        
print('Dataset contains {} pictures of {} digits \n'.format(len(objects), digits))

## Data preprocessing for S2 input ##

i_neur = []
t_neur = []

dur = num_nums_specific * digits #number of images in the input sequence
dur_total = (num_nums_specific + num_nums_specific_valid) * digits
spikes_dur = 250 #duration of a spike train for 1 picture (in ms) best - 250 ! last 62.5
pic_dur = spikes_dur * 2 #total time slot for 1 picture (in ms)


count = np.concatenate([np.arange(dur/2), np.arange(dur_total/2, dur_total/2 + dur/2)])
shuffle(count)

z = 0
for indx, cnt in enumerate(count):
    C1, C1num = pickles[int(cnt)]
    indices = array(range(0,C1num**2*6)) #indices of 6 arrays 25x25 for each orientation (flattened)
    times = array(C1.reshape((np.prod(C1.shape)))) / 1.
    times[times > spikes_dur] = 0 #to cut the spikes with too large latency (larger than 500 ms)
    j = 0
    while j < len(times):
        if times[j] == 0:
            times = np.delete(times, j)
            indices = np.delete(indices, j)
            j -= 1
        j+=1

    i_neur = np.concatenate([i_neur, indices])
    t_neur = np.concatenate([t_neur, times + z * pic_dur])
    z+=1
    if z >= dur:
        break
            
print('Number of pictures: ', z)

t_neur = t_neur * ms

def score(weights, pickles):

    freq = 8 * Hz
    amp_mod = 2 * mV#0.8
    taum = 10 * ms
    Ee = 0 * mV
    vt = -54 * mV
    vr = -60 * mV
    El = -74 * mV
    taue = 5 * ms

    i_neur_valid = []
    t_neur_valid = []

    valid = num_nums_specific_valid * digits

    count_valid = np.concatenate([np.arange(dur/2, dur_total/2), np.arange(dur_total/2 + dur/2, dur_total)])

    z = 0
    for cnt in count_valid:
            C1, C1num = pickles[int(cnt)]

            indices = array(range(0,C1num**2*6)) #indices of 6 arrays 25x25 for each orientation (flattened)
            times = array(C1.reshape((np.prod(C1.shape)))) / 1. #EDIT shrinking the spike train for correct STDP
            times[times > spikes_dur] = 0 #to cut the spikes with too large latency (larger than 500 ms)

            j = 0
            while j < len(times):
                if times[j] == 0:
                    times = np.delete(times, j)
                    indices = np.delete(indices, j)
                    j -= 1
                j+=1

            i_neur_valid = np.concatenate([i_neur_valid, indices])
            t_neur_valid = np.concatenate([t_neur_valid, times + z * pic_dur])
            z+=1
            if z >= dur:
                break

    t_neur_valid = t_neur_valid * ms

    test = Network()
    num_neurons = 10

    input_test = SpikeGeneratorGroup(C1num**2 * 6, i_neur_valid, t_neur_valid)
    test.add(input_test)

    neurons_test = NeuronGroup(num_neurons, 
                               '''dv/dt = (ge * (Ee-vr) + El - v) / taum : volt
                                          dge/dt = -ge / taue : 1''',
                          threshold='v>vt', reset='v = vr',
                          method='linear')
    neurons_test.v = vr
    test.add(neurons_test)

    S_test = Synapses(input_test, neurons_test, '''w : 1''', on_pre='''ge += w''')
    S_test.connect(p=1.0)
    S_test.w = weights
    test.add(S_test)


    cells = []
    for cnt in range(len(count_valid)):
        mon_test = SpikeMonitor(neurons_test, ['v'], record=True)
        test.add(mon_test)
        test.run(pic_dur * ms)
        # if cnt % 10 == 0:
            # print()
        # print('number processing: {}'.format(cnt))
        if len(mon_test.i) > 0:
          cells.append(mon_test.i[0])
        else:
          cells.append(-1)
        del mon_test

    count1 = np.empty((10), dtype=int)
    count2 = np.empty((10), dtype=int)
    for i in range(10):
        count1[i] = cells[:50].count(i)
        count2[i] = cells[50:].count(i)
    return (100 - np.sum(np.minimum(count1, count2)))

## S2 Layer ##
def network(noise, seed = 1):
    np.random.seed(seed)
    freq = 8 * Hz
    amp_mod = 2 * mV

    # Parameters
    # simulation_duration =  pic_dur / 1000 * dur * second #duration in seconds

    PoissonFreq = noise #0.8
    S2Inhibition = -25 * mV #-25

    ## Neurons
    taum = 10 * ms
    Ee = 0 * mV
    vt = -54 * mV
    vr = -60 * mV
    El = -74 * mV
    taue = 5 * ms

    ## STDP
    taupre = 100 * ms
    taupost = taupre
    gmax = .05
    dApre = .02
    dApost = -dApre * taupre / taupost * 1.05
    dApost *= gmax
    dApre *= gmax

    # Setting the stage
    network = Network()

    ## Stimuli section
    num_neurons = 10

    input_n = NeuronGroup(C1num**2 * 6, 'spike : 1', threshold='spike>0', reset='spike=0')
    input_n.spike = 0

    input_spikes = SpikeGeneratorGroup(C1num**2 * 6, i_neur, t_neur)
    stimulus = TimedArray(np.tile([PoissonFreq, PoissonFreq, 0., 0.], dur)*Hz, dt=spikes_dur/2*ms)
    input_Poisson = PoissonGroup(C1num**2 * 6, rates='stimulus(t)')

    network.add(input_n)
    network.add(input_spikes)
    network.add(input_Poisson)

    S_sp = Synapses(input_spikes, input_n, on_pre='spike+=1')
    S_sp.connect(j='i')
    network.add(S_sp)

    S_poi = Synapses(input_Poisson, input_n, on_pre='spike+=1')
    S_poi.connect(j='i')
    network.add(S_poi)

    neurons = NeuronGroup(num_neurons, '''dv/dt = (ge * (Ee-vr) + El - v + amp_mod * cos(2 * pi * (freq * t - 14/16))) / taum : volt
                                          dge/dt = -ge / taue : 1''',
                          threshold='v>vt', reset='v = vr',
                          method='euler')
    neurons.v = vr
    network.add(neurons)


    S = Synapses(input_n, neurons,
                 '''w : 1
                    dApre/dt = -Apre / taupre : 1 (event-driven)
                    dApost/dt = -Apost / taupost : 1 (event-driven)''',
                 on_pre='''ge += w
                        Apre += dApre
                        w = clip(w + Apost, 0, gmax)''',
                 on_post='''Apost += dApost
                         w = clip(w + Apre, 0, gmax)''',
                 )
    S.connect(p=1.0)

    k = 0
    for i, j in zip(S.i, S.j):
        k += 1

    smean = 0.7
    S.w = clip(np.random.normal(gmax * smean, gmax*0.1 / 5, size=k), (smean - 0.05) * gmax, (smean + 0.05) * gmax) #0.5 last (0.7!!)

    network.add(S)

    S_in = Synapses(neurons, neurons,
                       model='''s: volt''',
                       on_pre='v_post += s')
    S_in.connect(condition='i!=j')
    S_in.s = S2Inhibition

    network.add(S_in)
    syn_i = np.linspace(0, k-1, k)

    synapse_monitor = StateMonitor(S, 'w', record=True, dt=1*second) 
    network.add(synapse_monitor)

    scores = []

    for cnt in range(10):
        network.run(10 * second, report='text')
        current_score = score(S.w, pickles)
        scores.append(current_score)
        #print('score: ', current_score)

    return [synapse_monitor.w/gmax, synapse_monitor.t/second, scores]

i = 1
data = []
for n in np.linspace(0, 1.4, 29):
    data.append(network(noise=n, seed=i))
    i += 1

with open('local_data.pickle', 'wb') as f:
    pickle.dump(data, f)